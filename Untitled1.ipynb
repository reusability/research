{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import local modules.\n",
    "from model.least_squares import LeastSquares\n",
    "from model.tree_classifier import TreeClassifier\n",
    "from model.nerual_net import NN\n",
    "from model.k_nearest_neigbors import KNearestNeighbors\n",
    "from model.decision_tree import DecisionTree\n",
    "from model.support_vector_machine import SupportVectorMachine\n",
    "from model.lmnn import LMNN\n",
    "\n",
    "from utils import data_loader, pre_training_analysis_tools \n",
    "from utils.wknn_fs import WkNNFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 1. 2. 3. 1. 2. 1. 3. 2. 1. 2. 2. 1. 1. 1. 2. 2. 2. 1. 1. 1. 3. 1. 3.\n 1. 2. 2. 1. 1. 2. 2. 2. 1. 3. 2. 2. 2. 3. 3. 3. 2. 1. 2. 2. 1. 2. 3. 2.\n 1. 3. 2. 1. 3. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 3. 2. 2. 2. 2.\n 2. 1. 3. 2. 1. 1. 3. 2. 1. 2. 3. 2. 2. 2. 3. 1. 1. 3. 2. 1. 2. 2. 3. 2.\n 1. 2. 3. 2. 3. 2. 1. 3. 2. 2. 1. 1. 1. 2. 2. 1. 2. 3. 1. 3. 2. 1. 2. 1.\n 2. 2. 2. 2. 2. 3. 2. 2. 2. 2. 2. 1. 3. 1. 2. 1. 1. 2. 2. 1. 3. 1. 3. 1.\n 1. 1. 1. 1. 1. 3. 1. 2. 3. 1. 1. 2. 3. 1. 1. 1. 3. 2. 1. 2. 2. 3. 2. 1.\n 1. 1. 1. 1. 2. 2. 2. 3. 1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 1. 2. 3. 3. 3.\n 2. 2. 2. 2. 1. 2. 1. 3. 2. 2. 2. 1. 2. 2. 3. 3. 3. 2. 3. 2. 1. 3. 1. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9a1f68cc8a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;31m# necessary to uncomment for multi-class classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;31m# set parameters of transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \"\"\"\n\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     45\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 1. 2. 3. 1. 2. 1. 3. 2. 1. 2. 2. 1. 1. 1. 2. 2. 2. 1. 1. 1. 3. 1. 3.\n 1. 2. 2. 1. 1. 2. 2. 2. 1. 3. 2. 2. 2. 3. 3. 3. 2. 1. 2. 2. 1. 2. 3. 2.\n 1. 3. 2. 1. 3. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 3. 2. 2. 2. 2.\n 2. 1. 3. 2. 1. 1. 3. 2. 1. 2. 3. 2. 2. 2. 3. 1. 1. 3. 2. 1. 2. 2. 3. 2.\n 1. 2. 3. 2. 3. 2. 1. 3. 2. 2. 1. 1. 1. 2. 2. 1. 2. 3. 1. 3. 2. 1. 2. 1.\n 2. 2. 2. 2. 2. 3. 2. 2. 2. 2. 2. 1. 3. 1. 2. 1. 1. 2. 2. 1. 3. 1. 3. 1.\n 1. 1. 1. 1. 1. 3. 1. 2. 3. 1. 1. 2. 3. 1. 1. 1. 3. 2. 1. 2. 2. 3. 2. 1.\n 1. 1. 1. 1. 2. 2. 2. 3. 1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 1. 2. 3. 3. 3.\n 2. 2. 2. 2. 1. 2. 1. 3. 2. 2. 2. 1. 2. 2. 3. 3. 3. 2. 3. 2. 1. 3. 1. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "\"\"\"\n",
    "P. Bugata, P. Drotár, Weighted nearest neighbors feature selection,\n",
    "Knowledge-Based Systems (2018), doi:https://doi.org/10.1016/j.knosys.2018.10.004\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder    # needed for multi-class classification\n",
    "from sklearn.base import TransformerMixin\n",
    "import time\n",
    "\n",
    "from utils import data_loader, pre_training_analysis_tools\n",
    "\n",
    "\n",
    "class WkNNFeatureSelector(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, max_features, n_iters = 1000, n_iters_in_loop = 100, \n",
    "                 metric = 'euclidean', p = None, kernel = 'rbf', \n",
    "                 error_type = 'mse', delta = 1.0, \n",
    "                 lambda0 = 0.001, lambda1 = 0.001, lambda2 = 0.001, alpha = 100,\n",
    "                 optimizer = 'SGD', learning_rate = 0.1, \n",
    "                 normalize_gradient = True, data_type = 'float32', scaling = True,\n",
    "                 apply_weights = False, n_iters_weights = 300 , verbose = False                                \n",
    "        ):\n",
    "        # this class implements TransformerMixin interface\n",
    "        TransformerMixin.__init__(self)\n",
    "        \n",
    "        # how many features to select\n",
    "        self.max_features_ = max_features \n",
    "        # number of epochs\n",
    "        self.n_iters_ = n_iters\n",
    "        # number of iterations to display\n",
    "        self.n_iters_in_loop_ = n_iters_in_loop\n",
    "        # distance metric\n",
    "        self.metric_ = metric\n",
    "        # p norm used in minkowski distance\n",
    "        self.p_ = p\n",
    "        # kernel - distance evaluation function\n",
    "        self.kernel_ = kernel\n",
    "        # error function\n",
    "        self.error_type_ = error_type\n",
    "        # delta - used only for Huber Loss function\n",
    "        self.delta_ = delta\n",
    "        # regularization parameter for pseudo L0 regularization\n",
    "        self.lambda0_ = lambda0\n",
    "        # regularization parameter for L1 regularization\n",
    "        self.lambda1_ = lambda1\n",
    "        # regularization parameter for L2 regularization\n",
    "        self.lambda2_ = lambda2\n",
    "        # alpha - used only for pseudo L0 regularization\n",
    "        self.alpha_ = alpha\n",
    "        # optimizer type\n",
    "        self.optimizer_ = optimizer\n",
    "        # learning rate\n",
    "        self.learning_rate_ = learning_rate\n",
    "        # gradient normalization flag\n",
    "        self.normalize_gradient_ = normalize_gradient\n",
    "        # data type for precision and numerical stability\n",
    "        self.data_type_ = data_type\n",
    "        # standardization of input data\n",
    "        self.scl_ = None\n",
    "        if scaling:\n",
    "            self.scl_ = StandardScaler()\n",
    "        \n",
    "        # selected features\n",
    "        self.selected_features_ = None\n",
    "        # selected features after fine-tuning weights\n",
    "        self.final_selected_features_ = None\n",
    "        \n",
    "        # feature weights\n",
    "        self.weights_ = None\n",
    "        # feature weights after fine-tuning weights\n",
    "        self.final_weights_ = None       \n",
    "        \n",
    "        # error\n",
    "        self.error_ = None\n",
    "        # error after fine-tuning weights\n",
    "        self.final_error_ = None\n",
    "    \n",
    "        # checking parameters\n",
    "        # unsupported \n",
    "        if metric not in ['euclidean', 'cityblock', 'minkowski']:\n",
    "            raise ValueError('Unsupported metric')\n",
    "        if kernel not in ['rbf', 'exp']:\n",
    "            raise ValueError('Unsupported kernel')\n",
    "        if error_type not in ['mse', 'mae', 'huber', 'ce']:\n",
    "            raise ValueError('Unsupported error type')\n",
    "        if optimizer not in ['SGD', 'Adam', 'Nadam', 'Adagrad', 'Adadelta', 'RMSProp', 'Momentum']:\n",
    "            raise ValueError('Unsupported optimizer')\n",
    "        if data_type not in ['float32', 'float64']:\n",
    "            raise ValueError('Unsupported data type')\n",
    "        # unallowed    \n",
    "        if error_type == 'huber' and delta is None:            \n",
    "            raise ValueError('Parameter delta for Huber function is missing.')\n",
    "        if  alpha is None:            \n",
    "            raise ValueError('Parameter alpha for L0 regularization is missing.')\n",
    "        if optimizer != 'SGD' and self.normalize_gradient_:\n",
    "            raise ValueError('Gradient normalization is alloved only for SGD.')\n",
    "\n",
    "        # constant for numerical stability\n",
    "        if data_type == 'float32':\n",
    "            self.epsilon_ = tf.constant(1e-14, dtype='float32')\n",
    "        elif data_type == 'float64':    \n",
    "            self.epsilon_ = tf.constant(1e-300, dtype='float64')\n",
    "            \n",
    "        # classification task flag for multi-class classification\n",
    "        self.classification = None\n",
    "        \n",
    "        # flag to decide whether apply feature weights when transforming data\n",
    "        self.apply_weights = apply_weights\n",
    "        \n",
    "        # number of epochs to fine-tuning weights\n",
    "        self.n_iters_weights_ = n_iters_weights\n",
    "        \n",
    "        # debug prints\n",
    "        self.verbose = verbose\n",
    "\n",
    "    # pairwise sqeuclidean distance\n",
    "    def sqeuclidean_dist(self, A, B):  \n",
    "        # using matrix multiplication for efficient computation of Euclidean distance\n",
    "        with tf.variable_scope('squeclidean_dist'):\n",
    "          if not B is None:        \n",
    "              norm_a = tf.reduce_sum(tf.multiply(tf.square(A), self.weights_), 1)\n",
    "              norm_b = tf.reduce_sum(tf.multiply(tf.square(B), self.weights_), 1)\n",
    "              norm_a = tf.reshape(norm_a, [-1, 1])\n",
    "              norm_b = tf.reshape(norm_b, [1, -1])\n",
    "              scalar_product = tf.matmul(tf.multiply(A, self.weights_), B, False, True) \n",
    "          else:\n",
    "              norm_a = tf.reduce_sum(tf.multiply(tf.square(A), self.weights_), 1)\n",
    "              norm_b = norm_a\n",
    "              norm_a = tf.reshape(norm_a, [-1, 1])\n",
    "              norm_b = tf.reshape(norm_b, [1, -1])\n",
    "              scalar_product = tf.matmul(tf.multiply(A, self.weights_), A, False, True) \n",
    "        \n",
    "          D = norm_a - 2*scalar_product + norm_b\n",
    "\n",
    "          # special modification of distance matrix - use small value instead of 0\n",
    "          # to avoid computational problems when computing derivative\n",
    "          D = tf.where(D > self.epsilon_, D, tf.ones_like(D)*self.epsilon_)                 \n",
    "        \n",
    "        return D\n",
    "\n",
    "    # minkowski distance using map \n",
    "    def minkowski_dist_map(self, A, B):\n",
    "    \n",
    "        with tf.variable_scope('minkowski_dist'):\n",
    "            if B is None:\n",
    "                B = A\n",
    "             \n",
    "            def dist_(x):\n",
    "                result = tf.abs(tf.subtract(x, B))\n",
    "                    \n",
    "                if self.p_ != 1:\n",
    "                     result = tf.pow(result, self.p_)\n",
    "                    \n",
    "                result = tf.reduce_sum(tf.multiply(result, self.weights_), 1)        \n",
    "                    \n",
    "                if self.p_ != 1:                    \n",
    "                    # special modification of distance matrix - use small value instead of 0\n",
    "                    # to avoid computational problems when computing derivative\n",
    "                    result = tf.where(result > self.epsilon_, result, tf.ones_like(result)*self.epsilon_)                 \n",
    "                    \n",
    "                    result = tf.pow(result, 1/self.p_)\n",
    "                    \n",
    "                return tf.transpose(result)\n",
    "            \n",
    "            \n",
    "            D = tf.map_fn(fn=dist_, elems=A)\n",
    "                    \n",
    "        return D\n",
    "\n",
    "\n",
    "    # minkowski distance using expand_dim (slower then map_fn)\n",
    "    def minkowski_dist_expand(self, A, B):\n",
    "    \n",
    "        with tf.variable_scope('manhattan_dist'):            \n",
    "            if B is None:\n",
    "                B = A\n",
    "    \n",
    "            D = tf.abs(tf.subtract(A, tf.expand_dims(B, 1)))\n",
    "    \n",
    "            if self.p_ != 1:\n",
    "                D = tf.pow(D, self.p_)\n",
    "    \n",
    "            D = tf.reduce_sum(tf.multiply(D, self.weights_), axis=2)        \n",
    "    \n",
    "            if self.p_ != 1:\n",
    "                # special modification of distance matrix - use small value instead of 0\n",
    "                # to avoid computational problems when computing derivative\n",
    "                D = tf.where(D > self.epsilon_, D, tf.ones_like(D)*self.epsilon_)                 \n",
    "                \n",
    "                D = tf.pow(D, 1/self.p_)\n",
    "                    \n",
    "        return D\n",
    "\n",
    "    # using of nested map fn - slow!\n",
    "    def manhattan_dist(self, A, B):\n",
    "    \n",
    "        with tf.variable_scope('minkowski_dist'):\n",
    "            if B is None:\n",
    "                B = A\n",
    "             \n",
    "            def dist_(x):\n",
    "                \n",
    "                def dist_row_(y):\n",
    "                    result = tf.abs(tf.subtract(x, y))\n",
    "                    result = tf.reduce_sum(tf.multiply(result, self.weights_), 1)        \n",
    "                    return result\n",
    "\n",
    "                return tf.transpose(tf.map_fn(fn=dist_row_, elems=B))    \n",
    "            \n",
    "            D = tf.map_fn(fn=dist_, elems=A)\n",
    "                    \n",
    "        return D\n",
    "\n",
    "    # computing similarity matrix using corresponding metric and kernel\n",
    "    def similarity_matrix(self, A, B):\n",
    "    \n",
    "        similarity_mat = None\n",
    "        if (self.metric_ == 'euclidean'   and self.kernel_ == 'rbf'):            \n",
    "            dists = self.sqeuclidean_dist(A, B)\n",
    "            similarity_mat = tf.exp(-dists)\n",
    "    \n",
    "        if (self.metric_ == 'euclidean'   and self.kernel_ == 'exp'):           \n",
    "            dists = self.sqeuclidean_dist(A, B)                               \n",
    "            dists_sqrt = tf.sqrt(dists)\n",
    "            similarity_mat = tf.exp(-dists_sqrt)        \n",
    "    \n",
    "        if self.metric_ == 'cityblock':        \n",
    "            self.p_ = 1\n",
    "            dists = self.minkowski_dist_map(A, B)\n",
    "            if self.kernel_ == 'exp':\n",
    "                similarity_mat = tf.exp(-dists)\n",
    "            elif self.kernel_ == 'rbf':     \n",
    "                similarity_mat = tf.exp(-tf.square(dists))\n",
    "    \n",
    "        if self.metric_ == 'minkowski':        \n",
    "            if self.p_ is None:\n",
    "                raise ValueError('Parameter p is missing')\n",
    "            dists = self.minkowski_dist_map(A, B)                        \n",
    "            if self.kernel_ == 'exp':\n",
    "                similarity_mat = tf.exp(-dists)\n",
    "            elif self.kernel_ == 'rbf':     \n",
    "                similarity_mat = tf.exp(-tf.square(dists))\n",
    "    \n",
    "        if similarity_mat is None:\n",
    "            raise ValueError('Unsupported metric/kernel combination.')      \n",
    "    \n",
    "        return similarity_mat\n",
    "\n",
    "    # compute prediction vector\n",
    "    # prediction for i-th dataset point is weighted average of target values of other points\n",
    "    def predict(self, similarity_matrix, y):\n",
    "        zero_mat = tf.zeros_like(similarity_matrix)\n",
    "        diag_mat = tf.diag(tf.ones(y.shape.dims[0].value, dtype=y.dtype))\n",
    "        diag_mask = tf.greater(diag_mat, zero_mat)\n",
    "        mod_sim_matrix = tf.where(diag_mask, zero_mat, similarity_matrix)\n",
    "        \n",
    "        weight_sums = tf.reshape(tf.reduce_sum(mod_sim_matrix, 1), [-1,1])            \n",
    "\n",
    "        assert_op = tf.Assert(tf.reduce_all(weight_sums >= self.epsilon_), \n",
    "                              [tf.reduce_min(weight_sums)], name='assert_min_weigths') \n",
    "\n",
    "        with tf.control_dependencies([assert_op]):\n",
    "          predictions = tf.divide(tf.matmul(mod_sim_matrix, y), weight_sums)\n",
    "                   \n",
    "        return predictions  \n",
    "\n",
    "    # compute mean error - MSE, MAE, Huber loss or cross entropy\n",
    "    def compute_error(self, y, y_pred):\n",
    "        if self.error_type_ == 'mse':\n",
    "            losses = tf.square(tf.subtract(y, y_pred)) \n",
    "        elif self.error_type_ == 'mae':\n",
    "            losses = tf.abs(tf.subtract(y, y_pred))        \n",
    "        elif self.error_type_ == 'huber':\n",
    "            errors = tf.abs(tf.subtract(y, y_pred))\n",
    "            hlf_1 = 0.5 * tf.square(tf.subtract(y, y_pred))\n",
    "            hlf_2 = self.delta_ * errors - 0.5*self.delta_*self.delta_\n",
    "            losses = tf.where(tf.less_equal(errors, self.delta_), hlf_1, hlf_2)                 \n",
    "        elif self.error_type_ == 'ce':\n",
    "            # log(1 - abs(y - y_pred))\n",
    "            err_clipped = tf.maximum(1 - tf.abs(tf.subtract(y, y_pred)), self.epsilon_)\n",
    "            if self.classification:\n",
    "                losses = -tf.multiply(y, tf.log(err_clipped))\n",
    "            else:\n",
    "                losses = -tf.log(err_clipped)\n",
    "\n",
    "        # if y is one hot encoded, error is sum over all classes\n",
    "        if self.classification:\n",
    "            losses = tf.reduce_sum(losses, axis = 1)\n",
    "                            \n",
    "        return tf.reduce_mean(losses)\n",
    "\n",
    "    # pseudo L0 regularization\n",
    "    # sum of sigmoid functions indicating non-zero variable weight\n",
    "    def pseudo_l0_regularization(self):\n",
    "        reg = self.weights_\n",
    "        reg = tf.exp(reg*(-self.alpha_))\n",
    "        reg = tf.add(reg, tf.ones_like(reg))\n",
    "        reg = tf.divide(tf.ones_like(reg), reg)\n",
    "        reg = tf.add(reg, tf.ones_like(reg)*-1/2)\n",
    "        reg = tf.reduce_sum(reg)*2\n",
    "        return reg\n",
    "    \n",
    "    # L1 regularization to penalize sum of weights\n",
    "    def l1_regularization(self):\n",
    "        return tf.reduce_sum(self.weights_)\n",
    "    \n",
    "    # L2 regularization to penalize sum of squares of weights\n",
    "    def l2_regularization(self):\n",
    "        return tf.reduce_sum(tf.square(self.weights_))\n",
    " \n",
    "\n",
    "    # normalize gradient to max decrease by 1*learning_rate\n",
    "    def normalize_grad(self, grad, val):\n",
    "        # gradient L1 norm\n",
    "        gradient_norm = tf.reduce_sum(tf.abs(grad)) \n",
    "\n",
    "        # correction for negative weights\n",
    "        new_val = tf.subtract(val, grad * self.learning_rate_)\n",
    "        neg_sum = tf.reduce_sum(tf.minimum(new_val, 0.0))\n",
    "        gradient_norm = tf.add(gradient_norm, neg_sum / self.learning_rate_)\n",
    "        \n",
    "        # max 100x increase\n",
    "        gradient_norm = tf.maximum(gradient_norm, 0.01)\n",
    "\n",
    "        normalized_grad = tf.divide(grad, gradient_norm)                        \n",
    "        return normalized_grad\n",
    "\n",
    "        \n",
    "    # gradient clipping\n",
    "    def modified_minimize(self, optimizer, cost_function):\n",
    "        gvs = optimizer.compute_gradients(cost_function, var_list = [self.weights_])\n",
    "\n",
    "        if self.normalize_gradient_:\n",
    "            capped_gvs = [(self.normalize_grad(grad, val), val) for grad,val in gvs if grad is not None]                \n",
    "        else:\n",
    "            capped_gvs = [(tf.clip_by_norm(grad, 1.0), val) for grad,val in gvs if grad is not None]    \n",
    "            \n",
    "        apply_op = optimizer.apply_gradients(capped_gvs, name=\"apply_gradients\")    \n",
    "        \n",
    "        # after applying gradient clip negative weights\n",
    "        with tf.control_dependencies([apply_op]):      \n",
    "            assign_op = self.weights_.assign(tf.maximum(self.weights_, 0))\n",
    "            \n",
    "        return assign_op\n",
    "\n",
    "    # computing cost function\n",
    "    def compute_cost(self, X, y):\n",
    "        \n",
    "        # applying distance evaluation function\n",
    "        similarity_matrix_op = self.similarity_matrix(X, None)\n",
    "        \n",
    "        # predict target values\n",
    "        y_pred = self.predict(similarity_matrix_op, y)\n",
    "        \n",
    "        # compute error\n",
    "        mean_error = self.compute_error(y, y_pred)\n",
    "    \n",
    "        return mean_error\n",
    "\n",
    "\n",
    "    # optimizer loop in computation graph        \n",
    "    def optimizer_loop(self, optimizer, X, y, n_iter):\n",
    "    \n",
    "        # Use a resource variable for a true \"read op\"\n",
    "        with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "            var_err = tf.get_variable(name=\"var_last_err\", shape=[], use_resource=True, dtype=y.dtype)\n",
    "            var_reg0 = tf.get_variable(name=\"var_last_L0reg\", shape=[], use_resource=True, dtype=y.dtype)\n",
    "            var_reg1 = tf.get_variable(name=\"var_last_L1reg\", shape=[], use_resource=True, dtype=y.dtype)\n",
    "            var_reg2 = tf.get_variable(name=\"var_last_L2reg\", shape=[], use_resource=True, dtype=y.dtype)\n",
    "        \n",
    "        def _cond(i, _):\n",
    "            return tf.less(i, n_iter)  \n",
    "    \n",
    "        def _body(i, sequencer):\n",
    "    \n",
    "            mean_error = self.compute_cost(X, y)\n",
    "            modified_err = var_err.assign(mean_error)\n",
    "            \n",
    "            # add regularization   \n",
    "            if self.lambda0_ != 0 and self.lambda1_ != 0 and self.lambda2_ != 0:\n",
    "                reg0 = self.pseudo_l0_regularization()\n",
    "                reg1 = self.l1_regularization()\n",
    "                reg2 = self.l2_regularization()\n",
    "                modified_reg0 = var_reg0.assign(reg0)\n",
    "                modified_reg1 = var_reg1.assign(reg1)\n",
    "                modified_reg2 = var_reg2.assign(reg2)\n",
    "                cost = tf.add(mean_error, reg0 * self.lambda0_)\n",
    "                cost = tf.add(cost, reg1 * self.lambda1_)                                        \n",
    "                cost = tf.add(cost, reg2 * self.lambda2_)                                        \n",
    "            elif self.lambda0_ != 0 and self.lambda1_ != 0:\n",
    "                reg0 = self.pseudo_l0_regularization()\n",
    "                reg1 = self.l1_regularization() \n",
    "                modified_reg0 = var_reg0.assign(reg0)\n",
    "                modified_reg1 = var_reg1.assign(reg1)\n",
    "                modified_reg2 = var_reg2.assign(0)\n",
    "                cost = tf.add(mean_error, reg0 * self.lambda0_)\n",
    "                cost = tf.add(cost, reg1 * self.lambda1_)                \n",
    "            elif self.lambda0_ != 0 and self.lambda2_ != 0:\n",
    "                reg0 = self.pseudo_l0_regularization()\n",
    "                reg2 = self.l2_regularization() \n",
    "                modified_reg0 = var_reg0.assign(reg0)\n",
    "                modified_reg1 = var_reg1.assign(0)\n",
    "                modified_reg2 = var_reg2.assign(reg2)\n",
    "                cost = tf.add(mean_error, reg0 * self.lambda0_)\n",
    "                cost = tf.add(cost, reg2 * self.lambda2_)                \n",
    "            elif self.lambda1_ != 0 and self.lambda2_ != 0:\n",
    "                reg1 = self.l1_regularization()\n",
    "                reg2 = self.l2_regularization()\n",
    "                modified_reg0 = var_reg0.assign(0)\n",
    "                modified_reg1 = var_reg1.assign(reg1)\n",
    "                modified_reg2 = var_reg2.assign(reg2)\n",
    "                cost = tf.add(mean_error, reg1 * self.lambda1_)\n",
    "                cost = tf.add(cost, reg2 * self.lambda2_)                \n",
    "            elif self.lambda0_ != 0:\n",
    "                reg0 = self.pseudo_l0_regularization() \n",
    "                modified_reg0 = var_reg0.assign(reg0)\n",
    "                modified_reg1 = var_reg1.assign(0)\n",
    "                modified_reg2 = var_reg2.assign(0)\n",
    "                cost = tf.add(mean_error, reg0 * self.lambda0_)                \n",
    "            elif self.lambda1_ != 0:\n",
    "                reg1 = self.l1_regularization() \n",
    "                modified_reg0 = var_reg0.assign(0)\n",
    "                modified_reg1 = var_reg1.assign(reg1)\n",
    "                modified_reg2 = var_reg2.assign(0)\n",
    "                cost = tf.add(mean_error, reg1 * self.lambda1_)                \n",
    "            elif self.lambda2_ != 0:\n",
    "                reg2 = self.l2_regularization() \n",
    "                modified_reg0 = var_reg0.assign(0)\n",
    "                modified_reg1 = var_reg1.assign(0)\n",
    "                modified_reg2 = var_reg2.assign(reg2)\n",
    "                cost = tf.add(mean_error, reg2 * self.lambda2_)\n",
    "            else:                \n",
    "                cost = mean_error\n",
    "                modified_reg0 = var_reg0.assign(0)\n",
    "                modified_reg1 = var_reg1.assign(0)\n",
    "                modified_reg2 = var_reg2.assign(0)\n",
    "                    \n",
    "            train_op = self.modified_minimize(optimizer, cost)\n",
    "                        \n",
    "            with tf.control_dependencies([train_op, modified_err, modified_reg0, modified_reg1, modified_reg2]):\n",
    "                next_sequencer = tf.ones([])\n",
    "            return i + 1, next_sequencer\n",
    "        \n",
    "        init_err = var_err.assign(0.0)\n",
    "        init_reg0 = var_reg0.assign(0.0)\n",
    "        init_reg1 = var_reg1.assign(0.0)\n",
    "        init_reg2 = var_reg2.assign(0.0)\n",
    "        with tf.control_dependencies([init_err, init_reg0, init_reg1, init_reg2]):\n",
    "            _, sequencer = tf.while_loop(cond=_cond, body=_body, loop_vars=[0, 1.], parallel_iterations=1)\n",
    "    \n",
    "        with tf.control_dependencies([sequencer]):\n",
    "            last_err = var_err.read_value()\n",
    "            last_reg0 = var_reg0.read_value()\n",
    "            last_reg1 = var_reg1.read_value()\n",
    "            last_reg2 = var_reg2.read_value()\n",
    "    \n",
    "        return last_err, last_reg0, last_reg1, last_reg2\n",
    "\n",
    "    # creating optimizer\n",
    "    def create_optimizer(self):       \n",
    "        if self.optimizer_ == 'SGD':\n",
    "            return tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate_)\n",
    "        elif self.optimizer_ == 'Adam':\n",
    "            return tf.train.AdamOptimizer(learning_rate=self.learning_rate_)\n",
    "        elif self.optimizer_ == 'Nadam':\n",
    "            return tf.contrib.opt.NadamOptimizer(learning_rate=self.learning_rate_)\n",
    "        elif self.optimizer_ == 'Adagrad':\n",
    "            return tf.train.AdagradOptimizer(learning_rate=self.learning_rate_)\n",
    "        elif self.optimizer_ == 'Adadelta':\n",
    "            return tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate_)\n",
    "        elif self.optimizer_ == 'RMSProp':\n",
    "            return tf.train.RMSPropOptimizer(learning_rate=self.learning_rate_)\n",
    "        elif self.optimizer_ == 'Momentum':\n",
    "            return tf.train.MomentumOptimizer(learning_rate=self.learning_rate_, momentum=0.9, use_nesterov=True)\n",
    "                \n",
    "    # building model - select variables and determining their weights    \n",
    "    def fit(self, X, y, init_weights=None):\n",
    "\n",
    "        tf.logging.set_verbosity(tf.logging.DEBUG)    \n",
    "        \n",
    "        # data standardization\n",
    "        if self.scl_ is not None:\n",
    "            X_scl = self.scl_.fit_transform(X)\n",
    "        else:\n",
    "            X_scl = X\n",
    "\n",
    "        # create tensorflow graph\n",
    "        data_type = self.data_type_\n",
    "\t\t\n",
    "        # conversion of y to numpy\n",
    "        y = np.array(y)\n",
    "\t\t\n",
    "        # if y is 1D, reshape to column\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1,1)\n",
    "        \n",
    "        # classification flag is automatically set according to number of y columns      \n",
    "        self.classification = len(y[0]) > 1                   \n",
    "        \n",
    "        X_var = tf.constant(X_scl.astype(data_type), dtype=data_type, name='input_matrix')        \n",
    "        y_var = tf.constant(y.astype(data_type), dtype=data_type, name='target_var')\n",
    "\n",
    "        m = len(X[0])        \n",
    "        if init_weights is None:        \n",
    "            self.weights_ = tf.Variable(tf.ones([1,m], dtype=data_type)*1/m, \n",
    "                                        dtype=data_type, name='feature_weights')\n",
    "        else:\n",
    "            self.weights_ = tf.Variable(tf.convert_to_tensor(init_weights.astype(data_type)), \n",
    "                                        dtype=data_type, name='feature_weights')\n",
    "        \n",
    "        # creating and running optimizer\n",
    "        optimizer = self.create_optimizer()\n",
    "        optimizer_run = self.optimizer_loop(optimizer, X_var, y_var, self.n_iters_in_loop_)\n",
    "        \n",
    "        cost_op = self.compute_cost(X_var, y_var)        \n",
    "        reg_op0 = self.pseudo_l0_regularization()    \n",
    "        reg_op1 = self.l1_regularization()    \n",
    "        reg_op2 = self.l2_regularization()    \n",
    "                            \n",
    "        end_opt = False\n",
    "        \n",
    "        # creating session and evaluation\n",
    "        config = tf.ConfigProto()\n",
    "        # don't pre-allocate memory\n",
    "        config.gpu_options.allow_growth = True\n",
    "        # create a session with specified option\n",
    "        with tf.Session(config=config) as sess:\n",
    "            if self.verbose:\n",
    "                print('Start')\n",
    "    \n",
    "            # global variable initialization\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            error = None\n",
    "            if self.n_iters_  < self.n_iters_in_loop_:\n",
    "                self.n_iters_in_loop_ = self.n_iters_\n",
    "            steps = (int) (self.n_iters_ / self.n_iters_in_loop_)\n",
    "            \n",
    "            for e in range(steps):\n",
    "                try:                \n",
    "                    error, reg0, reg1, reg2 = sess.run(optimizer_run)\n",
    "                    if self.verbose:\n",
    "                        print('Epoch:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L0 reg:', reg0, 'L1 reg:', reg1, 'L2 reg:', reg2)\n",
    "    \n",
    "                except Exception as ex:\n",
    "                    print('Exception:', ex.__class__, ex.__context__)\n",
    "                    end_opt = True\n",
    "\n",
    "                if (end_opt or error is None or np.isnan(error)):\n",
    "                    break\n",
    "                                                           \n",
    "            # print final error\n",
    "            if not end_opt and self.verbose:\n",
    "                if self.lambda0_ != 0 and self.lambda1_ != 0 and self.lambda2_ != 0:   \n",
    "                    error, reg0, reg1, reg2 = sess.run([cost_op, reg_op0, reg_op1, reg_op2])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L0 reg:', reg0, 'L1 reg:', reg1, 'L2 reg:', reg2)\n",
    "                elif self.lambda0_ != 0 and self.lambda1_ != 0:   \n",
    "                    error, reg0, reg1 = sess.run([cost_op, reg_op0, reg_op1])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L0 reg:', reg0, 'L1 reg:', reg1)\n",
    "                elif self.lambda0_ != 0 and self.lambda2_ != 0:   \n",
    "                    error, reg0, reg2 = sess.run([cost_op, reg_op0, reg_op2])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L0 reg:', reg0, 'L2 reg:', reg2)\n",
    "                elif self.lambda1_ != 0 and self.lambda2_ != 0:   \n",
    "                    error, reg1, reg2 = sess.run([cost_op, reg_op1, reg_op2])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L1 reg:', reg1, 'L2 reg:', reg2)\n",
    "                elif self.lambda0_ != 0:   \n",
    "                    error, reg0 = sess.run([cost_op, reg_op0])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L0 reg:', reg0)\n",
    "                elif self.lambda1_ != 0:   \n",
    "                    error, reg1 = sess.run([cost_op, reg_op1])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L1 reg:', reg1)\n",
    "                elif self.lambda2_ != 0:   \n",
    "                    error, reg2 = sess.run([cost_op, reg_op2])\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error, 'L2 reg:', reg2)\n",
    "                else:\n",
    "                    error = sess.run(cost_op)\n",
    "                    print('Final:', (e+1) * self.n_iters_in_loop_, 'error:', error)\n",
    "                self.error_ = error\n",
    "\n",
    "            # final variable weights\n",
    "            self.weights_ = np.abs(self.weights_.eval()).flatten()\n",
    "            \n",
    "            # release resources\n",
    "            sess.close()\n",
    "\n",
    "            self.selected_features_ = self.weights_.argsort()[::-1]                         \n",
    "            nonzero_count = len(self.weights_[self.weights_ > 0])\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Non-zero weights: ', nonzero_count)\n",
    "                print('Big weights: ', len(self.weights_[self.weights_ > 0.001]))\n",
    "                print('Weight sum: ', self.weights_.sum())\n",
    "            \n",
    "            self.selected_features_ = self.selected_features_[:min(self.max_features_, nonzero_count)] \n",
    "\n",
    "            # additional fine-tuning weights\n",
    "            if not self.apply_weights:\n",
    "                return self\n",
    "\n",
    "            weights = self.weights_\n",
    "            selected_features = self.selected_features_\n",
    "            n_iters = self.n_iters_\n",
    "            scl = self.scl_\n",
    "            error = self.error_\n",
    "            \n",
    "            self.n_iters_ = self.n_iters_weights_\n",
    "            self.scl_ = None\n",
    "            self.apply_weights = False\n",
    "            X_transformed = X_scl[:,selected_features]\n",
    "            if self.verbose:\n",
    "                print('Selected features: ', selected_features.tolist())            \n",
    "                print('Selected weights: ', weights[selected_features].tolist())            \n",
    "                print('Selected weights sum: ', weights[selected_features].sum())            \n",
    "            self.fit(X_transformed, y, weights[selected_features])\n",
    "            self.final_selected_features_ = selected_features[self.selected_features_]\n",
    "            self.final_weights_ = np.zeros(shape=m, dtype=weights.dtype)\n",
    "            self.final_weights_[self.final_selected_features_] = self.weights_[self.selected_features_]\n",
    "            self.final_error_ = self.error_\n",
    "            if self.verbose:\n",
    "                print('Final selected features: ', self.final_selected_features_.tolist())            \n",
    "                print('Final selected weights: ', self.final_weights_[self.final_selected_features_].tolist())            \n",
    "                        \n",
    "            self.weights_ = weights\n",
    "            self.selected_features_ = selected_features\n",
    "            self.scl_ = scl\n",
    "            self.n_iters_ = n_iters\n",
    "            self.apply_weights = True\n",
    "            self.error_ = error\n",
    "            return self\n",
    "                                   \n",
    "\n",
    "    # transforming data by variable selection and/or applying weights\n",
    "    def transform(self, X):\n",
    "        # data standardization\n",
    "        if self.scl_ is not None:\n",
    "            X_transformed = self.scl_.transform(X)\n",
    "        else:\n",
    "            X_transformed = X\n",
    "\n",
    "        # applying variable weights to transformed data\n",
    "        if self.apply_weights:\n",
    "            X_transformed = X_transformed[:,self.final_selected_features_] \n",
    "            # correct application of weights according to metric                        \n",
    "            weights_to_use = self.final_weights_[self.final_selected_features_]\n",
    "            if self.metric_.count('minkowski') > 0 and self.p_ > 1:\n",
    "                weights_to_use = weights_to_use**(1/self.p_)\n",
    "            elif self.metric_.count('euclidean') > 0:\n",
    "                weights_to_use = np.sqrt(weights_to_use)\n",
    "            X_transformed = np.multiply(X_transformed, weights_to_use)\n",
    "        else:\n",
    "            X_transformed = X_transformed[:,self.selected_features_]             \n",
    "        \n",
    "        return X_transformed\n",
    "            \n",
    "                \n",
    "if __name__ == '__main__':    \n",
    "    # because of reproducible results\n",
    "    #tf.set_random_seed(1)\n",
    "    import pandas as pd\n",
    "\n",
    "    data = data_loader.load_real_dataset()\n",
    "    \n",
    "    x = data['train_x']\n",
    "    y = data['train_y']\n",
    "\n",
    "    #concat two dataframes for better visualization \n",
    "    y.reset_index(drop=True)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    matrix = pd.concat([x,y],axis=1)\n",
    "    \n",
    "    x = matrix\n",
    "    y = matrix.pop('maven_reuse')\n",
    "            \n",
    "    # options for initial weights\n",
    "    #m = len(X[0])\n",
    "    m = 208\n",
    "    v_m = np.ones([1,m])*1/m\n",
    "    v_zero = np.zeros([1,m])\n",
    "\n",
    "    # necessary to uncomment for multi-class classification\n",
    "    y = OneHotEncoder(sparse=False).fit_transform(y)    \n",
    "\n",
    "    # set parameters of transformer\n",
    "\n",
    "    # max_features - number of features to select\n",
    "    # n_iters - number of iterations (epochs)\n",
    "    # n_iters_in_loop - number of iterations to display progress\n",
    "    # metric - definition of distance (euclidean, cityblock, minkowski)\n",
    "    # p - parameter for Minkowski distance\n",
    "    # kernel - distance evaluation function (rbf, exp)\n",
    "    # error_type - error function (mse, mae, huber, ce)\n",
    "    # delta - parameter of Huber loss function\n",
    "    # lambda0, lambda1, lambda2 - regularization parameter for pseudo L0, L1, and L2 regularization\n",
    "    # alpha - parameter for pseudo L0 regularization (steepness of sigmoid function)\n",
    "    # optimizer - optimizer type (SGD, Adam, Nadam, Adagrad, Adadelta, RMSProp, Momentum)\n",
    "    # learning_rate - parameter for gradient descent\n",
    "    # normalize_gradient - flag for L1 normalization of gradient\n",
    "    # data_type - data type for controlling precision\n",
    "    # scaling - flag for standardization of input data\n",
    "    # apply_weights - flag for using weights for transformation of entire dataset\n",
    "    # n_iters_weights - number of iterations for fine-tuning weights\n",
    "    # verbose - boolean flag indicating whether print messages\n",
    "    \n",
    "    transformer = WkNNFeatureSelector(\n",
    "                    max_features = 50, n_iters =10000, n_iters_in_loop = 1000, \n",
    "                    metric = 'euclidean', p = 2, kernel = 'exp', \n",
    "                    error_type = 'ce', delta = 0.1, \n",
    "                    lambda0 = 0.00, lambda1 = 0.00, lambda2 = 0.00, alpha = 100,\n",
    "                    optimizer = 'SGD', learning_rate = 0.1, \n",
    "                    normalize_gradient = True, data_type = 'float64', scaling = True,\n",
    "                    apply_weights = False, n_iters_weights = 1, verbose = False)                                 \n",
    "\n",
    "    t_start = time.time()    \n",
    "\n",
    "    # X, y - input data\n",
    "    # init_weights - set initial weights\n",
    "    transformer.fit(X, y, init_weights=v_m)\n",
    "\n",
    "    col_indices = transformer.selected_features_    \n",
    "    feature_weights = transformer.weights_[col_indices]\n",
    "    print('Variables: ', col_indices.tolist())\n",
    "    print('Variable weights: ', feature_weights.tolist())\n",
    "\n",
    "    t_end = time.time()    \n",
    "    print(\"Duration:\", t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit7f1e84062742452e83d5ee685770f253"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
